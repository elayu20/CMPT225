üîπ Slides 2‚Äì4: What and Why of Algorithm Analysis

1. What are the two main ways we measure algorithm efficiency?
	Space and time efficiency

2. What questions do we ask when evaluating time efficiency?
	How long, in ms, does algorithm take to solve problem
	How much time increases as problem size increases
	How does it compare to other algorithms

3. What questions do we ask when evaluating space efficiency?
	How much memory space does my algorithm require to solve problems?

4. Why can the choice of algorithm make a system significantly more or less usable?
	Some algorithms become slower as it grows
	Efficient ones allow larger-scale systems

5. Give real-world examples where algorithm performance matters.
	Systems that need instant responses
	- air traffic control systems
	- competitive video games

üîπ Slides 5‚Äì7: Comparing and Timing Algorithms

6. Why might different algorithms solve the same problem in different ways?
	Produces the same results but different efficiency

7. Why are we usually interested in algorithm performance on large inputs?
	On small inputs, usually algorithm will perform well
	On larger inputs, performance differs much more

8. What makes recursive Fibonacci inefficient?
	It repeatedly computes the values everytime it recurses, which is exponentially growing in time complexity

9. How can you count how many operations an algorithm performs?
	Visually walkthrough code
	Insert code in algorithm to print the count

10. Why might timing an algorithm not accurately measure its efficiency?
	There are other several factors that can affect its time
	- different hardware parts
	- OS
	- which programming language
	- how algorithm is implemented
	- system tasks
	- other programs

11. List some external factors that affect the measured run-time of an algorithm.
	CPU speed, amount of Ram, etc

üîπ Slides 8‚Äì12: Counting and Cost Functions

12. What‚Äôs the benefit of counting operations instead of timing?
	The hardware isn't a factor when measuring efficiency
	We focus purely on the algorithm and its efficiency/growth rate

13. What does it mean to represent running time as a cost function t‚Çê(n)?
	We can use an input of size n instead of a fixed size
	t(n) = 3n + 2
	- Declare and initialize i is 1 operation (1)
	- Compare, print elements, and increment are 3 operations done n times (n + n + n)
	- Compare when i = n (1)
	Therefore, 1 + n + n + n + 1 can be written is 3n + 2

14. Write the cost function for printArray() and explain how it‚Äôs calculated.
	t(n) = 1 + n + n + n + 1 = 3n + 2

15. How can you use a ‚Äúcount‚Äù variable to measure operation counts in code?
	Add a pass-by-reference count parameter, then initialize count variable
	Then, increment operation counts, only count algorithm operations, NOT profiling code

16. What assumptions are made when counting each C++ statement as one operation?
	Every C++ statement = 1 operation
	All operations take equal time

17. Why are those assumptions not entirely true?
	Not every C++ statement is a operation, like a function call
	Some operations take different time, some are faster

18. What is a barometer instruction (or key operation)?
	It's the operation/instruction that is executed the most amount of times
	Usually it's the same as its running time

19. Why is the barometer instruction useful for estimating cost?
	Because the growth rate is the same as the instruction executed the most amount of times

üîπ Slides 13‚Äì14: Input Cases

20. What are the best, average, and worst cases for algorithm performance?
	These describe how performance differs based on different inputs of the same size, so t(n)

21. Why is the best case often not important in practice?
	Because we want to optimize our code, and we want to find a way to optimize the worst way

üîπ Slides 15‚Äì24: Linear Search

22. How does linear search work?
	Start at beginning of list, keep going up the list till find match or list ends

23. What is returned when the item is found? When it is not found?
	Returns index where item is found, or -1 when not found

24. What are the three barometer instructions in linear search?
	arr[i] == x
	i < n
	i++

25. How many comparisons occur in the best, worst, and average cases?
	Best case: target is first element of array, 1 comparison
	Worst case: target not in array, or is last element of array. n comparisons
	Average case: (3n + 1)/4	

26. What assumptions (A1 and A2) are used when calculating the average cost?
	Assumption 1: target is not in array half the time
	Assumption 2: equal probability of target being at location if in array, so a target in an array has a 1/n chance of being at location i

27. Derive the expected number of comparisons, E = (n + 1)/2.
	(1 + 2 + 3 + ... + n)/n = (n + 1)/2

28. Using the assumptions, compute the overall average cost (3n + 1)/4.
	Target not in array/last element of array + Target is in array
	n + (n + 1)/2
	2n/2 + (n+1)/2
	(3n+1)/2 = worst case + target in array
	Average = (worst case + target in array)/2
	(3n+1)/2/2 = (3n+1)/4

29. How does the recursive version of linear search work?
	Base case is when found target, or reach end of array
	Recursive case is when target not found

30. How does sorting an array change the average cost of linear search?
	It changes the average cost of linear search to n / 2, so it only looks at half the array
	This is because once an element in array is greater than or equal to target, then there's no point in looking at the other half of array

üîπ Slides 25‚Äì33: Binary Search

31. How does binary search differ from linear search?
	Array must be sorted
	Finds the middle element and sees if target matches middle element
	If not, determines if target is greater than or less than middle element
	If greater than, then only look at right side of array
	If less than, then only look at left side of array

32. What three main parts make up the algorithm?
	Initializing the lower and upper elements as variables
	While loop with return of middle index if target found
	Return statement (usually after while loop) which indicates failure (i.e. didn't find target)

33. What are the barometer instructions in the while loop?
	low <= high
	mid = (low + high) / 2
	x == arr[mid]
	x > arr[mid] OR x < arr[mid]

34. What is the best case for binary search?
	When middle of initial array is target

35. What is the worst case, and how many iterations does it require?
	Worst case: target not in array
	Also worst case: search space only has one element (low and high are the same index)
	Requires log[2](n) + 1 iterations in the worst case

36. Explain why the worst case occurs when n = 2·µè ‚áí k = log‚ÇÇn.
	Since each iteration halves the search space of n, then where k is the kth iteration, n/2^k represents space after kth iteration until size 1
	When in the worst case, kth iteration is the worst case, so worst case iterations -> k = log[2](n)

37. Why is the average case more similar to the worst case?
	Target is usually found near the end of the search
	This is because chance of array element being the target is initially 1/n, then 1/(n/2) on second iteration
	So as more iterations happen, 1/n, 1(n/2), 1(n/3), 1(n/k), then it is most likely to find the target near end

38. Write the recursive version of binary search and identify its base cases.
	base case: if (start > end), return -1 // error with arguments because start index is greater than last index,target not found
	2nd base case: if (arr[mid] == x), return mid index // target is found

39. Compare linear and binary search in terms of operations for various n.

üîπ Slides 35‚Äì50: Selection Sort

40. Describe how selection sort divides the array into sorted and unsorted parts.
	Repeatedly find smallest element's index
	Swap with first unsorted element
	Mark the swapped portion as sorted
	Repeat, until array is fully sorted

41. What two main operations does selection sort perform repeatedly?
	getSmallest and swap

42. What does getSmallest() do? How many operations does it perform?
	Returns the smallest index of unsorted array
	Performs 4(end-start-1) + 4 operations

43. How many swaps does selection sort perform overall?
	n - 1 swaps

44. How do you calculate the total cost of nested loops?
	average number of iterations of inner loop * iterations of outer loop
	total = sum of iterations of inner loop

45. Derive the sum of the sequence 1 + 2 + ‚Ä¶ + (n ‚Äì 1).
	sum = (number_of_values * (first + last))/2
	sum = ((n-1) * (1 + n - 1)) / 2 -> ((n-1) * n) / 2
	sum = (n^2 - n)/2

46. Combine the operations to find the total cost function t‚Çõ‚Çë‚Çó‚Çëc‚Çú·µ¢‚Çí‚Çô = 2n¬≤ + 8n ‚Äì 8.
	Total cost = (3n-3) + (2n^2 + 2n - 4) + (3n-1)
	Cost function: 2n^2 + 2n + 3n + 3n -4 -3 -1 = 2n^2 + 8n - 8

47. Which instruction is the barometer operation for selection sort?
	i < end
	arr[i] < arr[smallest]
	i++

48. How many times is it executed?
	n(n-1)/2

49. How does the organization of the input (best/worst case) affect selection sort?
	Only affects whether smallest is reassigned; doesn't change number of comparisons

50. Why is the difference between best and worst case small for this algorithm?
	Difference between 3n^2 and 4n^2, so only affects a few assignments, not loop counts

üîπ Slides 51‚Äì59: Insertion Sort

51. How does insertion sort differ conceptually from selection sort?
	It builds the sorted section gradually by inserting each new element into its correct position

52. What does it mean to ‚Äúexpand the sorted part‚Äù?
	Move elements in sorted part up one position until correct position for first unsorted element is found

53. In the insertion example, how many comparisons and moves occur for inserting 21?
	27

54. Write out the insertion sort code and identify its barometer operations.
	pos > 0 && arr[pos -1] > temp

55. What is the worst-case input arrangement?
	Array is in descending order [5, 4, 3, 2, 1]

56. Derive the total worst-case comparisons/moves = n(n ‚Äì 1)/2.
	n(n-1)/2 comparisons and same number of moves

57. What is the best-case arrangement? How many operations occur?
	Array is already sorted is best case
	n comparisons, no moves, linear time O(n)

58. What is the average case, and how is it estimated?
	Closer to worst case: n*(n-1)/4 comparisons from random input

üîπ Slides 60‚Äì70: Seven Important Functions

59. List the seven key growth functions used in algorithm analysis.
	Constant O(c); c a constant
	Logarithmic O(logn)
	Linear O(n)
	O(nlogn)
	Quadratic O(n^2)
	Polynomial O(n^k); k >= 0
	Exponential O(k^n); k > 1

60. Give an example of an algorithm for each (O(1), O(log n), O(n), etc.).
	Constant - O(1): swap two values in array
	Logarithmic - O(logn): Binary search
	Linear - O(n); search for index in array, linear search, traverse linked list
	O(nlogn): sorting - mergesort, heap sort, quick sort
	Quadratic - O(n^2): simple sorting - bubble sort, selection sort, insertion sort
	Polynomial O(n^k): matrix multiplication
	Exponential O(k^n): recursive fibonacci

61. What makes a function logarithmic? What‚Äôs the usual base in CS?
	Log functions are log base b of n
	The usual base is 2

62. Apply logarithm rules to simplify: log‚ÇÇ(2n), log‚ÇÇ(n/2), log‚ÇÑn, 2À°·µí·µç¬≤‚Åø.
	log[2](2n) = log[2](2) + log[2](n) = 1 + log[2](n)
	log[2](n/2) = log[2](n) - log[2](2) = log[2](n) - 1
	log[4](n) = log[2](n) / log[2](4) = log[2](n) / log[2](2^2) = log[2](n) / 2
	2^(log[2](n)) = n^(log[2](2)) = n^1 = n

63. What types of algorithms produce n log n behavior?
	Sorting algorithms such as mergesort, heap sort, quicksort

64. What makes a function quadratic?
	Output is input^2
	Contains nested loops

65. Define a polynomial function formally.
	f(n) = a[0] + a[1](n) + a[2]n^2 + a[3]n^3 + ... + a[d]n^d
	where a[0], a[1], ..., a[d] are constants, called coefficients, of the polynomial and a[d] != 0

66. What‚Äôs meant by polynomial time and why is it desirable?
	Algorithms with complexity <= n^k for constant k; generalyl considered efficient

67. What defines an exponential function, and why are such algorithms intractable?
	b^n, b is often 2, b > 1
	They are intractable because very slow

üîπ Slides 71‚Äì91: Big-O and Related Notations

68. Why do we analyze growth rates as n ‚Üí ‚àû?
	Want to compare algorithms behavior for large input sizes, so as n gets large
	Focusing on long-term growth, not short-term performance

69. Why do we often ignore leading constants and lower-order terms?
	Constants are usually insignificant, and do not really affect growth rate for large n

70. Using the cost function 2n¬≤ + 8n ‚Äì 8, explain which term dominates and why.
	n^2 dominates because it is the dominating term for large input values of n

71. Define O-notation formally using constants c and n‚ÇÄ.
	f(n) is O(g(n)) if there is a real constant c > 0 and an integer constant n[o] such that f(n) <= cg(n) for all n >= n[o]

72. In plain English, what does ‚Äúf(n) is O(g(n))‚Äù mean?
	f(n) is order of g(n) or f(n) is in O(g(n))
	The algorithm's running time grows no faster than some constant multiple of g(n)

73. Demonstrate with a graph how 3n¬≤ ‚â• 2n¬≤ + 8n ‚Äì 8 for n ‚â• 8.
	3(8)^2 = 192
	2(8)^2 + 8(8) - 8 = 184
	192 >= 184

74. Find c and n‚ÇÄ that prove 3n + 12 is O(n).
	f(n) = 3n + 12	
	3n + 12 <= c * n; n >= n[o] // Divide both sides by n
	3 + 12/n <= c
	For n >= n[o], 3 + 12/n <= 3 + 12/n[o]
	We could say c = 3 + 12/n[o]
	Choose n[o] = 12
	c = 3 + 12/n[o] -> c = 3 + 12/12 -> c = 4
	3n + 12 <= 4n
	Therefore, 3n = 12 = O(n) with c = 4 & n[o] = 12

75. Show how to find c and n‚ÇÄ that prove 2n¬≤ + 8n ‚Äì 8 is O(n¬≤).
	2n^2 + 8n - 8 <= c * n^2, n >= n[o]
	2 + 8/n - 8/n^2 <= c, n >= n[o]
	2 + 8/n - 8/n^2 <= 2 + 8/n[o] - 8/(n[o])^2; n = 4, n[o] = 2 works
	c = 2 + 8/n[o] - 8/(n[o])^2 -> c = 2 + 8/(2) - 8/(2^2) = 2 + 4 - 2 = 4
	2n^2 + 8n - 8 <= 4n^2
	Therefore, 2n^2 + 8n - 8 = O(n^2), with c = 4 & n[o] = 2

76. Simplify examples:

O(23 * log n)
	O(logn)

O(n + n¬≤)
	max[O(n), O(n^2)] = O(n^2)

77. List and explain arithmetic rules for Big-O (O(kf), O(f+g), O(f*g)).
	O(kf) = O(f) if k is a constant
	O(f + g) = max[O(f), O(g)]
	O(f * g) = O(f) * O(g)

78. When might an algorithm‚Äôs growth rate vary with input organization?
	Depends on input arrangement - e.ge, sorting partially sorted vs reverse-orderd data

79. Distinguish between O (Big-O), Œ© (Big-Omega), and Œò (Big-Theta).
	Big-O gives the upper bound
	Big-Omega gives a lower bound (there exists a constant c such that c * f(n) is a lower bound on the cost function g(n))
	Big-Theta gives an upper and lower bound (k1 * f(n) is an upper bound on g(n) and k2 * f(n) is a lower bound on g(n))

80. What are common growth rate categories and their relative speeds?
	O(1) < O(logn) < O(n) < O(nlogn) < O(n^2) < O(n^3) < O(n^k) < O(2^n)
